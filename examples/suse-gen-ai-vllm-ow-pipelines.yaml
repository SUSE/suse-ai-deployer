# This is an example overrides yaml:
# - enabling deployment of vllm with GPU with a basic configuration.
# - enabling deployment of open-webui along with pipelines.
# - enabling deployment of milvus in cluster mode.
# - services deployed with suse-gen-ai release name in suse-private-ai namespace.
# - longhorn storage.
# Please update these overrides as needed for your environment.

global:
  imagePullSecrets:
  - application-collection
ollama:
  #To learn more about the entries, helm show values oci://dp.apps.rancher.io/charts/ollama
  enabled: false
open-webui:
  #To learn more about the entries, helm show values oci://dp.apps.rancher.io/charts/open-webui
  enabled: true
  persistence:
    enabled: true
    size: 20Gi
    storageClass: longhorn
  ollama:
    enabled: false
  pipelines:
    enabled: true
    persistence:
      storageClass: longhorn
    extraEnvVars:
    - name: PIPELINES_URLS
      value: "https://raw.githubusercontent.com/SUSE/suse-ai-observability-extension/refs/heads/main/integrations/oi-filter/conversation_turn_limit_filter.py"
  ingress:
    host: suse-ollama-webui
  extraEnvVars:
  - name: DEFAULT_USER_ROLE
    value: "user"
  - name: ENABLE_SIGNUP
    value: "true"
  - name: GLOBAL_LOG_LEVEL
    value: INFO
  - name: RAG_EMBEDDING_MODEL
    value: "sentence-transformers/all-MiniLM-L6-v2"
  - name: VECTOR_DB
    value: "milvus"
  - name: MILVUS_URI
    value: http://suse-gen-ai-milvus.suse-private-ai.svc.cluster.local:19530
  - name: INSTALL_NLTK_DATASETS
    value: "true"
  - name: OPENAI_API_BASE_URLS
    value: "http://open-webui-pipelines.suse-private-ai.svc.cluster.local:9099;http://suse-gen-ai-router-service.suse-private-ai.svc.cluster.local:80/v1"
  - name: OPENAI_API_KEYS
    value: "0p3n-w3bu!;dummy"
milvus:
  #To learn more about the entries, helm show values oci://dp.apps.rancher.io/charts/milvus
  enabled: true
  cluster:
    enabled: true
  etcd:
    replicaCount: 1
    persistence:
      storageClassName: longhorn
  minio:
    persistence:
      storageClass: longhorn
      size: 20Gi
    resources:
      requests:
        memory: 1024Mi
  kafka:
    enabled: true
    persistence:
      enabled: true
      storageClassName: longhorn
pytorch:
  enabled: false
vllm:
  #To learn more about the entries, helm show values oci://dp.apps.rancher.io/charts/vllm
  enabled: true
  servingEngineSpec:
    modelSpec:
    - name: "phi3-mini-4k"
      registry: "dp.apps.rancher.io"
      repository: "containers/vllm-openai"
      tag: "0.9.1"
      imagePullPolicy: "IfNotPresent"
      modelURL: "microsoft/Phi-3-mini-4k-instruct"
      replicaCount: 1
      requestCPU: 6
      requestMemory: "16Gi"
      requestGPU: 1

